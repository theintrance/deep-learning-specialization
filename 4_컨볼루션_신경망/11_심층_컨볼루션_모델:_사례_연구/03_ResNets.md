---
title: ResNets
---

### Residual Block

$$
z^{[l+1]} = W^{[l+1]} a^{[l]} + b^{[l+1]}\\
a^{[l+1]} = g(z^{[l+1]})\\

z^{[l+2]} = W^{[l+2]} a^{[l+1]} + b^{[l+2]}\\
a^{[l+2]} = g(z^{[l+2]})
$$

- 위와같은 신경망이 있다고 할때 아래와 같이 요약할 수 있음
    1. $a^{[l]}$
    2. Linear
    3. ReLU
    4. $a^{[l+1]}$
    5. Linear
    6. ReLU
    7. $a^{[l+2]}$
- 이 일련의 과정을 Main Path 라고 한다.
- 여기서 $a^{[l]}$ 을 5번 Linear 이후 더해주어 중간 레이어를 스킵할 수 있고, 이를 Short cut 혹은 Skip connecttion 이라고 한다.
    - → $a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$
    - 이는 기울기 소실 문제를 해결할 수 있다.
    - 네트워크가 더 깊어지더라도 학습이 잘 진행된다.