---
title: ResNet이 필요한 이유는?
---
?
### 리셋이 필요한 이유는?

- 어떤 NN 에 추가적으로 2개의 레이어를 추가하고 Skip Connection 을 도입한 경우:
    - $a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$
    - $= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$

- 위 식에서 $W^{[l+2]}, b^{[l+2]}$ 이 모두 0이라고 가정하면 a$^{[l+2]}=g(a^{[l]})$ 이고 $g(x)=ReLU(x)$ 라면 $g(a^{[l]})=a^{[l]}$ 이 된다.
- 즉 추가한 2개의 레이어가 항등함수로써 작동하게 되므로 이 경우 다른 레이어에 영향을 끼치지 않게 된다.

- **항등함수의 의미: 안전 장치**
    - Residual Block은 네트워크에 안전 장치를 제공. 만약 추가된 레이어가 네트워크의 성능을 개선하지 못하거나 오히려 저해한다면, 신경망은 항등함수(즉, 추가된 레이어를 무시하는)를 학습하게 됨.
    - 이렇게 함으로써 신경망이 학습할 때 추가된 레이어가 필요한지 아닌지를 스스로 판단할 수 있음. 즉, 레이어를 추가해두었더라도, 이 레이어가 유용하지 않다면 스킵되어 전체 성능을 저하시키지 않도록 함.
- **Residual Block의 핵심: 네트워크의 유연성 증가**
    - Residual Block은 추가된 레이어가 유익할 경우 더 복잡한 패턴을 학습하고, 그렇지 않은 경우 레이어를 무시할 수 있는 유연성을 제공.
    - 이런 구조 덕분에 신경망은 더 깊어질 수 있으며, 필요한 경우에만 복잡한 패턴을 학습하고 그렇지 않은 경우 항등함수로 돌아감. 이 유연성 덕분에 매우 깊은 네트워크에서도 안정적인 학습이 가능.

![](/assets/fe9f3175-888c-415f-89c5-4e75ce421c75.png)

- 이것을 도입하려면 $z^{[l+2]}$ 의 차원과 $a^{[l]}$ 의 차원이 동일해야한다.
- 즉 Same Convolution Layer 에서 유용하며, Pooling 등의 사용으로 차원이 바뀔 때 $W_s$ 같은 차원 변경을 위한 매트릭스를 도입하여 사용하기도 한다.