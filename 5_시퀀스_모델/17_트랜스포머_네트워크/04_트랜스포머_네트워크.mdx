---
title: 트랜스포머 네트워크
---

### Transformer Network

![](/assets/35ce393a-d394-40ec-8c5a-c0a624d5cdc7.png)

1. 인코더(Encoder)와 디코더(Decoder) 구조
   - 트랜스포머 네트워크는 **인코더-디코더 구조**를 사용함.
     - **인코더(Encoder)**:
       - 입력 문장을 처리하여 각 단어의 의미를 **벡터로 변환**.
       - 단어 간의 문맥적 관계를 학습하고, 이를 고차원 벡터로 표현.
       - **6개의 블록**으로 구성되며, 각 블록은 다음으로 이루어짐:
         - **셀프 어텐션(Self-Attention)**: 문장 내 모든 단어 간의 상호 관계를 학습.
         - **피드포워드 신경망(Feed-Forward Network)**: 각 단어의 표현을 비선형 변환하여 더욱 복합적인 정보로 처리.
     - **디코더(Decoder)**:
       - 인코더의 출력과 번역된 문장의 단어들을 바탕으로 **다음 단어를 예측**.
       - **6개의 블록**으로 구성되며, 다음 두 가지 어텐션 메커니즘을 사용:
         - **셀프 어텐션(Self-Attention)**: 디코더가 이미 번역한 단어들 간의 관계를 학습.
         - **크로스 어텐션(Cross-Attention)**: 인코더의 출력과 디코더의 상태를 사용하여 입력 문장과 번역된 문장 간의 관계를 학습.
       - 디코더는 **한 번에 하나씩 단어를 예측**하고, 매 예측된 단어를 다시 입력으로 받아 다음 단어를 생성함.

2. 포지셔널 인코딩(Positional Encoding)
   - 트랜스포머는 RNN처럼 순차적으로 데이터를 처리하지 않기 때문에, **단어 순서 정보**를 인식할 방법이 필요함.
     - 이를 해결하기 위해 **포지셔널 인코딩**을 사용.
     - 각 단어의 **위치 정보를 벡터로 인코딩**하여, 입력 임베딩에 더해짐.
       - 사인(sine)과 코사인(cosine) 함수로 각 단어의 위치를 고유하게 표현.
       - 이를 통해 트랜스포머는 문장의 단어들이 어떤 **순서로 배치**되었는지를 학습 가능.

3. 잔여 연결(Residual Connection)과 정규화(Layer Normalization)
   - 트랜스포머는 각 층(layer)에서 **잔여 연결**을 사용하여 이전 층의 출력을 현재 층의 입력에 더함.
     - **잔여 연결**은 깊은 네트워크에서 발생할 수 있는 **기울기 소실 문제**를 해결하고 학습을 안정화함.
   - 또한, 각 층에서 **Layer Normalization**을 적용하여 출력값을 정규화함.
     - 이는 학습을 가속화하고, 각 층에서의 출력을 **안정적으로 유지**하는 데 기여함.

4. 마스크 멀티 헤드 어텐션(Masked Multi-Head Attention)
   - 디코더는 한 번에 한 단어씩 번역을 예측하지만, 훈련 중에는 전체 문장을 이미 알고 있음.
     - **마스크(Masking)**는 훈련 중에도 디코더가 **미래의 단어**를 보지 못하게 함.
       - 이를 통해, 디코더는 이전 단어만을 참조하여 다음 단어를 예측하게 됨.
     - 이 마스킹은 테스트 중에 **단어를 하나씩 예측하는 방식**과 동일한 환경을 만들기 위함.
   - 마스크 멀티 헤드 어텐션은 디코더의 셀프 어텐션 단계에서 마스킹이 적용된 어텐션 메커니즘을 사용함.