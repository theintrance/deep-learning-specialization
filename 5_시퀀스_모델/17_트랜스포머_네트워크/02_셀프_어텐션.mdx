---
title: 셀프 어텐션
---

### Self-Attention 직관

$$
A(q, k, v)
$$
* 위 식은 쿼리 벡터 $q$, 키 벡터 $k$, 값 벡터 $v$ 를 입력으로 받아 어텐션 기반 벡터 표현을 반환하는 함수를 의미한다.

* 각 입력에 대한 어텐션 값의 벡터라고 생각할 수 있다.

$$
\begin{matrix}
\text{Jane} & \text{visits} & \text{l'Afrique} & \text{en} & \text{Septembre} \\
\uparrow & \uparrow & \uparrow & \uparrow & \uparrow \\
A^{<1>} & A^{<2>} & A^{<3>} & A^{<4>} & A^{<5>}
\end{matrix}
$$

* 트랜스포머 셀프 어텐션 방정식:

$$
\text{Attention}(q, K, V) = \sum_i\frac{\exp(q \cdot k^{<i>})}{\sum_j \exp(q \cdot k^{<j>})} v^{<i>}
$$
 
* 이제 $A^{<3>}$ 을 구해보자.

$$
q^{<3>} = W^Q \cdot x^{<3>} \\
k^{<3>} = W^K \cdot x^{<3>} \\
v^{<3>} = W^V \cdot x^{<3>}
$$

* $A^{<3>}$ 을 구하기 위해서는 모든 입력에 대한 쿼리, 키, 값 벡터를 계산해야 하고, 각 값은 입력과 특정 가중치 행렬을 곱한 결과이다.
* 이 가중치는 학습의 대상 (파라미터) 이다.
1. $q$ 는 모든 입력의 $k$ 와 곱해지고, 여기서는 입력이 5개이므로 5개의 내적 결과를 바탕으로 Softmax 를 계산한다.
2. 소프트맥스 결과에 $v$ 를 곱한다.
3. 모두 더하여 $A^{<3>}$ 을 얻는다.

* 계산 과정에서, $q^{3}\cdot k^{<2>}$ 이 소프트맥스에서 제일 큰 값이 되게 된다.
    * 이는 $\text{visite}=x^{<2>}$ 의 목적이 $\text{l'Afrique}=x^{<3>}$ 이기 때문에 높은 어텐션 값을 가진다고 할 수 있다. (서로의 관계성이 높다)