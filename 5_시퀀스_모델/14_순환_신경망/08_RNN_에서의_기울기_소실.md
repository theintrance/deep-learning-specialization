---
title: RNN 에서의 기울기 소실
---

### RNN과 기울기 소멸 문제

- **기본 RNN의 문제점**: RNN은 문맥을 기억하여 언어 모델링과 같은 문제를 해결하지만, **기울기 소멸(Gradient Vanishing)** 문제로 인해 긴 시퀀스에서 장기적인 의존성을 포착하기 어렵다.
    - 예시: 문장에서 "The cat was full"과 같이 문법적으로 **단수형**인지 **복수형**인지에 대한 정보를 유지해야 하는 경우, 기본 RNN은 이를 학습하기 어렵다.
    - RNN은 순전파(왼쪽에서 오른쪽)와 역전파(오른쪽에서 왼쪽)를 통해 학습하지만, **시간이 길어질수록 초기 정보가 나중의 예측에 영향을 미치는 것이 어려워진다**.
- **기울기 소멸**: RNN의 역전파 과정에서 시간이 길어질수록 기울기(Gradient)가 매우 작아지면서 초기에 입력된 정보가 후반부 예측에 영향을 미치기 힘들어진다.
    - 이는 **깊은 신경망**에서 발생하는 문제와 유사하다. 깊은 네트워크에서는 뒤쪽의 출력이 초기 레이어에 미치는 영향을 점점 잃어버리게 된다.
    - 기울기 소멸로 인해, RNN은 주로 **최근 정보**에만 의존하게 되며, 시퀀스 초반의 정보는 잘 기억하지 못한다.
- **기울기 폭주(Gradient Explosion)**: 기울기가 **기하급수적으로 커지는 현상**도 발생할 수 있다.
    - 이는 기울기 값이 너무 커져서 매개변수가 폭발(NaN 값)하는 문제를 일으킨다.
    - 이 문제는 그래디언트 클리핑(Gradient Clipping)을 통해 해결할 수 있다. 클리핑은 기울기 벡터의 크기가 임계값을 넘으면, 해당 벡터를 최대값에 맞춰 조정하는 방식이다.
- **RNN의 한계**: 기본 RNN은 **장기적인 의존성(Long-Term Dependency)**을 잘 포착하지 못한다. 이는 시퀀스 초반의 정보가 후반부 예측에 미치는 영향이 줄어들기 때문이다.
    - 기울기 소멸 문제로 인해, RNN은 **로컬 의존성**(가까운 단어 간의 관계)에만 주로 의존하게 된다.