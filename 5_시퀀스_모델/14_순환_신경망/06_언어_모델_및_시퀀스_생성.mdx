---
title: 언어 모델 및 시퀀스 생성
---

- **언어 모델**은 주어진 문장이 나올 확률을 예측하는 모델이다.
    - 예:
        - $P(\text{REAL MADRID THE BEST})=3.2 \times 10^{-13}$
        - $P(\text{FC BAYERN THE BEST})=5.7 \times 10^{-10}$
    - 두 문장의 확률 차이는 모델이 후자의 문장을 더 가능성 있는 문장으로 판단하고 있음을 의미한다.
- **RNN**을 언어 모델로 학습하기 위해, 입력 문장을 **Tokenize**하여 처리해야 한다.
    - Tokenize는 문장을 단어 또는 문자로 분리하고, 각 단어를 **딕셔너리**에서 찾아 **원핫 벡터**로 변환하는 작업을 말한다.
    - 문장의 끝을 알리기 위해 **EOS** 토큰을 추가한다.
    - 딕셔너리에서 찾을 수 없는 단어는 **unk** 토큰으로 처리된다.
    
    > 예시: Cats average 15 hours of sleep a day. EOS
    > 
- **RNN의 시퀀스 생성 과정**:
    - $x^{<1>}$은 문장의 시작을 의미하는 특별한 입력 벡터로 $\vec{0}$으로 채워진다. 이 벡터를 첫 번째 RNN 유닛에 입력하면 첫 번째 단어 "Cats"의 확률 분포 $\hat{y}^{<1>}$이 출력된다.
    - 두 번째 RNN 유닛의 입력 $x^{<2>}$는 첫 번째 유닛의 예측 값 $y^{<1>}$, 즉 "Cats"가 된다. 이 과정이 반복되며 각 시간 단계에서 다음 단어를 예측한다.
    - 즉, $x^{<t+1>} = y^{<t>}$이며, RNN은 각 단어를 순차적으로 예측한다.
- **손실 함수**는 다음과 같이 정의된다:
    - 각 시간 단계 $t$에서의 손실 $\mathcal{L}^{<t>}$:
    
    $\mathcal{L}(\hat{y}^{<t>}, y^{<t>}) = -\sum_i y_i^{<t>} \log{\hat{y}_i^{<t>}}$
    - 전체 시퀀스에 대한 손실 $\mathcal{L}$:
    
    $\mathcal{L} = \sum_t \mathcal{L}^{<t>}(\hat{y}^{<t>}, y^{<t>})$
    - 이 손실 함수는 **Cross-Entropy Loss**로, 실제 단어 $y^{<t>}$ (원핫 벡터로 표현됨)와 예측된 확률 $\hat{y}^{<t>}$ 간의 차이를 계산하여 모델의 성능을 평가한다.
- **핵심 요약**:
    - RNN은 이전 단어의 예측 값을 다음 단어의 입력으로 사용하여 시퀀스를 생성한다.
    - 문장의 끝을 EOS로 처리하고, 사전에 없는 단어는 unk로 처리한다.
    - 손실 함수는 실제 값과 예측 값 간의 차이를 Cross-Entropy Loss로 계산하며, 각 시간 단계의 손실을 합산하여 최종 손실 값을 구한다.