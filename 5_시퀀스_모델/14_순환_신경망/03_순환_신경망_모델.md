---
title: 순환 신경망 모델
---

### 왜 일반적인 네트워크는 안 될까?

1. 시퀀스는 Input 과 Output 의 길이가 달라질 수 있다.
    1. 일반적인 네트워크는 크기가 고정되어있다.
2. 텍스트 시퀀스의 서로 다른 위치에서 학습한 기능을 공유하지 않는다.

### Recurrent Neural Networks, RNN

- 문장의 첫번째 단어 $x^{<1>}$ 가 네트워크를 지나 $\hat{y}^{<1>}$ 를 출력한 후 $x^{<2>}$ 를 연산할 때 지난 단어의 활성인 $a^{<1>}$ 가 사용된다.
- $x^{<t>}$ 가 네트워크를 지나면서 레이어가 셀에 피드백하는 루프가 있을 수 있다.
- $x^{<t>}$ 에서 $x^{<t+1>}$ 로의 연결을 관리하는 매개변수 집합을 $W_{ax}$ 라 하고 $W_{aa}$, $W_{ya}$ 도 사용한다.

![](/assets/93a9e788-bee1-4365-b1eb-dcf32ce03cd9.png)

- RNN 에서 $x^{<1>}$ 이 $\hat{y}^{<3>}$ 에 영향을 끼칠 수 있으나 $\hat{y}^{<3>}$ 은 $x^{<4>}$ 의 영향은 받지 않는다.
    - 양방향 RNN 이 이를 해결할 수 있다.

### RNN forward prop

$$
a^{<0>}=\vec{0}
$$

$$
a^{<t>}=g(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)\\
\hat{y}^{<t>}=g(W_{ya}a^{<t>}+b_y)\\

$$

- 축약 표기:

$$
a^{<t>}=g(W_{a}[a^{<t-1>},x^{<t>}]+b_a)

$$

$$
W_a=[W_{aa}\vdots{}W_{ax}]
$$

$$
[a^{<t-1>}, x^{<t>}]=\begin{bmatrix}a^{<t-1>}\\\overline{x^{<t>}}\end{bmatrix}
$$

$$
[W_{aa}\vdots{}W_{ax}]\begin{bmatrix}a^{<t-1>}\\\overline{x^{<t>}}\end{bmatrix}=W_{aa}a^{<t-1>}+W_{ax}x^{<t>}
$$