### 기존 RNN의 문제

> He said, "Teddy bears are on sale!"
> He said, "Teddy Roosevelt was a great President!"

* 기존 RNN은 위 문장에서 "Teddy"가 사람 이름인지 판단하는 데 어려움이 있다.
* RNN은 문장을 순차적으로 처리하기 때문에 "Teddy"를 처리할 때, 앞부분("He", "said")만 보고 결정을 내리게 된다.
* 하지만 두 번째 문장에서는 뒤에 오는 "Roosevelt"가 "Teddy"가 사람 이름임을 알려준다. RNN은 이러한 미래의 정보를 활용할 수 없기 때문에, 이 문제를 해결하기 위해 양방향 RNN을 사용한다.

### 양방향 RNN (Bidirectional RNN)

![](https://miro.medium.com/v2/resize:fit:1218/1*c4yHwRQESwG5e0WsmW-zBw.gif)
[출처](https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0)

* 양방향 RNN은 기존 RNN의 각 유닛에 Forward 유닛과 Backward 유닛을 추가한다. 이렇게 하면 모델은 문장의 앞쪽과 뒤쪽 모두에서 정보를 가져올 수 있게 되어, 더 정확한 예측이 가능하다.
* 양방향 RNN의 출력값 \(\hat{y}^{<t>}\)는 순방향 (\(\overrightarrow{a^{<t>}}\))과 역방향 (\(\overleftarrow{a^{<t>}}\))의 hidden state를 모두 결합하여 계산된다.

$$
\hat{y}^{<t>} = g(W_y[\overrightarrow{a^{<t>}}, \overleftarrow{a^{<t>}}] + b_y)
$$

* 여기서 \(\overrightarrow{a^{<t>}}\)는 t 시점까지 순방향으로 처리된 정보이고, \(\overleftarrow{a^{<t>}}\)는 t 시점까지 역방향으로 처리된 정보이다. 이 두 정보를 결합함으로써 문장의 전체 문맥을 고려할 수 있다.
* Forward와 Backward 유닛은 LSTM 또는 GRU로 구현될 수 있다.

### 양방향 RNN의 단점

* 양방향 RNN은 문장의 전체 문맥을 고려할 수 있지만, 모델의 복잡도가 증가하고 연산량이 증가한다.
  * 순방향과 역방향 RNN을 모두 사용하기 때문에 파라미터 수가 두 배가 되어, 메모리 사용량과 학습 시간이 증가한다.
* 연산을 하기 전에 전체 데이터 시퀀스가 필요하다.
  * 실시간 데이터 처리나 스트리밍 데이터와 같은 시나리오에서는 전체 시퀀스를 미리 알 수 없으므로, 양방향 RNN을 사용하기 어렵다.