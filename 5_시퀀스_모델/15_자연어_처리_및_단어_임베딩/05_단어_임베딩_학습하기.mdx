---
title: 단어 임베딩 학습하기
---

### Neural Language Model

|Word:|I|want|a|glass|of|orange|____.
|---|---|---|---|---|---|---|---
|Index:|4343|9665|1|3852|6163|6257|?

* 문장의 마지막에 올 단어를 예측해보자.

$$
x^{<1>} = \text{"I"} \rightarrow o_{4343} \rightarrow E \cdot o_{4343} = e_{4343}
$$

$$
x^{<2>} = \text{"want"} \rightarrow o_{9665} \rightarrow E \cdot o_{9665} = e_{9665}
$$

$$
x^{<3>} = \text{"a"} \rightarrow o_{1} \rightarrow E \cdot o_{1} = e_{1}
$$

$$
x^{<4>} = \text{"glass"} \rightarrow o_{3852} \rightarrow E \cdot o_{3852} = e_{3852}
$$

$$
x^{<5>} = \text{"of"} \rightarrow o_{6163} \rightarrow E \cdot o_{6163} = e_{6163}
$$

$$
x^{<6>} = \text{"orange"} \rightarrow o_{6257} \rightarrow E \cdot o_{6257} = e_{6257}
$$

* 임베딩 테이블 $E$ 가 10000 x 300 차원이라면, $e_{index}$ 는 300 차원의 벡터가 된다.

* 이렇게 각 단어의 임베딩 벡터를 신경망에 넣고 10000개의 출력을 가지는 softmax 층을 통과시키면 다음에 나오는 단어를 예측할 수 있다.

* 지금 설명된 과정에서 두 가지가 동시에 일어나고 있다:

1. **임베딩 벡터 학습:**
   - 임베딩 벡터 $E$ 는 처음에는 랜덤으로 설정되지만, 학습 과정에서 점차 의미 있는 값으로 업데이트된다.
   - 문장 속 단어들을 예측하면서 임베딩 벡터도 학습된다. 즉, 단어를 예측하는 과정이 임베딩 벡터 학습을 도와준다.

2. **문장에서 다음 단어 예측:**
   - 언어 모델은 주어진 문맥을 바탕으로 다음에 올 단어를 예측한다.
   - 단어들이 임베딩 벡터로 변환되어 뉴럴 네트워크에 입력되고, softmax를 통해 가장 가능성 높은 단어를 선택하여 예측한다.

이 두 과정은 서로 연결되어 있으며, 단어 예측을 통해 임베딩 벡터가 점점 더 정확하게 학습된다.

### 더 긴 문장에서의 단어 예측

* 문장이 길어질 수록 단어를 예측할 때 문맥 (Context) 를 잘 이해해야 한다.
    * 예를 들어 orange juice 에서 juice 를 예측할 때는 orange 가 문맥이 된다.

* 언어 모델이 단어를 예측할 때 사용할 문맥의 범위를 설정할 수 있다.
1. 앞 4개 단어
2. 앞 8개 단어
3. 앞 뒤 각각 4개 단어
4. 앞 1개 단어 (Skip-Gram)

* 등등 다양하게 설정할 수 있다.