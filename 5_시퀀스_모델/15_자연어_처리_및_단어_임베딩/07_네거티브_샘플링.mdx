---
title: 네거티브 샘플링
---

$$
p(t \mid c) = \frac{e^{\theta_t^Te_c}}{\sum_{j=1}^{V} e^{\theta_j^Te_c}}
$$

* 지난 시간에 살펴본 소프트맥스 함수를 사용하는 모델에서는 분모의 합을 계산하는 과정에서 계산 비용이 매우 높다는 문제가 있었다.
* 이를 해결하기 위해 계층적 softmax 말고 또 다른 방법인 네거티브 샘플링을 사용할 수 있다.

$$
\text{I want a glass of orange juice to go along with my cereal.}
$$

* 위 예문에서 orange 를 context 단어로 선정하였다고 가정하고, $V$ 에 있는 단어들에 대해 target 으로 적절한지 판단하는 테이블을 만들어보자.

|context|word|target?|
|---|---|---|
|orange|juice|1|
|orange|king|0|
|orange|book|0|
|orange|the|0|
|orange|fo|0|

* 테이블에서 알 수 있듯이 orange 의 target 단어로는 juice 가 적절하고, king, book, the, fo 등은 적절하지 않다.
* 이 테이블을 바탕으로 context 단어와 target 단어 하나를 입력으로 받아 target 인지 아닌지 (y) 를 예측하는 이진 분류 모델을 만들 수 있다.

$$
P(y=1 \mid c, t)=\sigma(\theta_t^Te_c)
$$

* 위 테이블처럼 orange 의 target 이 될 수 있는 단어가 juice 밖에 없고, target 이 될 수 없는 단어가 $k(=4)$ 개 있다고 가정한다.
* Orange 를 뜻하는 $O_{6257}$ 에서 $e_{6257}$ 를 구하고, 이를 10,000개 특성을 가지는 softmax 함수에 넣는 것이 아니라 10,000개의 이진 분류 유닛에 넣을 수 있다.
    * 또한 이 10,000개의 이진 분류기를 학습시킬 때 $k$ 에서 몇 개 단어를 샘플링하고, target 이 될 수 있는 1개의 예시와 함께 학습시킨다.
    * 이렇게 하면 계산 비용을 크게 줄일 수 있다.
    * 이렇게 부정적 예시 $k$ 개와 긍정적 예시 1개를 함께 학습시키는 것을 네거티브 샘플링이라고 한다.


#### 그럼 네거티브 샘플은 어떻게 선정할 수 있을까?

* 네거티브 샘플링을 할 때 모든 단어를 대상으로 하는 것이 아니라 빈도수를 기반으로 선정할 수 있다.

$$
P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=1}^{V} f(w_j)^{3/4}}
$$

* 위 식에서 $f(w_i)$ 는 단어 $w_i$ 의 빈도수를 의미한다.
* 이 식에 따라 빈도수가 높은 단어는 높은 확률로 뽑히고, 빈도수가 낮은 단어는 낮은 확률로 뽑힌다.