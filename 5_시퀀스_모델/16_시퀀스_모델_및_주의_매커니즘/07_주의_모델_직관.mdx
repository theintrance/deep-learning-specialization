---
title: 어텐션 모델 직관
---

### 긴 시퀀스에 대한 문제

* 우리는 지금까지 RNN 에서 encoder 와 decoder 의 구조를 이용하여 번역 모델을 구축하였다.
* encoder 는 번역 대상 시퀀스를 모두 읽고 이를 압축하여 하나의 벡터로 표현하고, decoder 는 이를 받아 차례대로 번역을 생성한다.
* 하지만 번역 대상 시퀀스가 길어질 경우, encoder 의 마지막 출력은 초기 입력에 대한 정보를 상당히 잃어버리게 된다.


### 어텐션 모델의 직관

![](/assets/8ba39ee0-3b83-4735-81c4-cc13dcf43491.png)

* 위 그림은 어텐션 모델의 직관을 보여준다.
* 번역 대상 시퀀스의 각 단어에 대한 중요도를 계산하고, 이를 바탕으로 번역을 생성한다.
