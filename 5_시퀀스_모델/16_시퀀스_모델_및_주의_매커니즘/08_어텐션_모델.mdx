---
title: 어텐션 모델
---

### 어텐션 모델

* 어텐션 메커니즘은 RNN 기반 모델에서 중요한 단어를 더 집중적으로 처리하여 번역, 요약 등 다양한 작업에서 성능을 향상시킨다.

![](/assets/29c43420-fc7f-4ac1-87c9-223e22c26465.png)

* 어텐션 모델에서 **양방향 RNN**을 구현하기 위해 각 GRU (또는 LSTM) 유닛은 **순방향 은닉 상태** $\overrightarrow{a^{<t>}}$ 와 **역방향 은닉 상태** $\overleftarrow{a^{<t>}}$ 를 계산한다.
* 이를 결합하여 $a^{<t>}=(\overrightarrow{a^{<t>}}, \overleftarrow{a^{<t>}})$ 로 표기하며, 이 은닉 상태들이 어텐션 가중치 계산의 중요한 입력이 된다.
* 번역 대상 시퀀스의 인덱스를 $t'$ 로 표기한다.

#### 컨텍스트 벡터 계산하기

* 어텐션 메커니즘은 입력 시퀀스 $a^{<t'>}$가 출력 시퀀스에서 얼마나 중요한지를 나타내는 가중치 $\alpha^{<t, t'>}$ 를 계산한다.
* 이 가중치를 바탕으로 **컨텍스트 벡터** $c^{<t>}$ 가 계산된다.

$$
\sum_{t'} \alpha^{<t, t'>} = 1
$$

$$
c^{<t>} = \sum_{t'} \alpha^{<t, t'>} a^{<t'>}
$$

* 컨텍스트 벡터 $c^{<t>}$ 는 입력 시퀀스 전체에서 중요한 정보를 종합한 벡터로, 이를 통해 어텐션 모델의 각 유닛이 다음 단어를 예측하는 확률 분포 $\hat{y}^{<t>}$ 를 계산한다.
* 모델의 입력은 **이전 은닉 상태** $s^{<t-1>}$, **컨텍스트 벡터** $c^{<t>}$, **이전 출력** $\hat{y}^{<t-1>}$ 이다.

### 어텐션 가중치 $\alpha^{<t, t'>}$ 계산하기

* $\alpha^{<t, t'>}$는 출력 시퀀스에서 단어 $t$를 예측할 때, 입력 단어 $t'$에 할당된 중요도를 나타낸다.

$$
\left.
\begin{matrix}
s^{<t-1>}\\
a^{<t'>}
\end{matrix}
\right)\rightarrow\text{Small NN}\rightarrow e^{<t, t'>}
$$

* **작은 신경망**은 **이전 은닉 상태** $s^{<t-1>}$와 **입력 시퀀스 은닉 상태** $a^{<t'>}$를 결합하여 점수 $e^{<t, t'>}$ 를 계산한다.
    * 이 점수는 각 입력 단어가 출력에 얼마나 기여하는지를 평가한 값이다.
* 그런 다음 **softmax 함수**를 사용하여 가중치 $\alpha^{<t, t'>}$ 를 계산한다:

$$
\alpha^{<t, t'>} = \frac{\exp(e^{<t, t'>})}{\sum_{t'=1}^{T_x} \exp(e^{<t, t'>})}
$$

* $\alpha^{<t, t'>}$ 값은 각 입력 단어가 출력 단어에 미치는 영향력을 나타내며, 모든 $\alpha^{<t, t'>}$ 의 합은 1이 된다.