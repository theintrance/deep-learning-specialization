---
title: 정규화
---
### 정규화 (Regularization)

- 정규화는 고분산(과적합, overfitting) 의 해결 방법 중 하나임
- 더 많은 훈련 세트를 확보하는 것이 제일 좋겠으나 항상 이것이 가능하지는 않음. 이럴 때 시도해 볼 수 있는 방법이다.
- 정규화를 도입하기 위해 방법에 따라 비용 함수 $J$ 를 다음과 같이 정의한다.

### L2 정규화

$$
J(w, b) = \frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat y^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_2^2\\||w||_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw
$$

### L1 정규화

$$
J(w, b) = \frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(\hat y^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_1\\||w||_1=\sum_{j=1}^{n_x}|w|
$$

- 보통 L1 보다 L2가 많이 사용됨
- 여기서 $\lambda$ 는 정규화 계수 (Regaularization Parameter) 로써 하이퍼 파라미터 중 하나임

### 레이어 $l$ 에 대한 가중치 정규화

$$
J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}, \dots,w^{[l]}, b^{[l]}) = \frac{1}{m}\sum_{i=1}^{m}L(\hat y^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2_F
$$

$$
||w^{[l]}||^2_F=\sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij})^2
$$

- 식은 어려워 보이지만 단순히 $w^{[l]}$ 의 모든 요소들의 제곱의 합
- 가중치 벡터의 크기가 크면 비용 함수의 값이 커진다 → 즉 경사 하강은 $w$, $b$ 의 값이 감소하는 방향으로 진행된다.
- $||\centerdot||_F$ 을 Frobenius Norm 이라고 한다.
(Norm: 벡터의 크기)

### 정규화의 경사 하강, 가중치 업데이트

$$
dw^{[l]}=\text{(from backprop)}+\frac{\lambda}{m}w^{[l]}
$$

- 기존 역전파를 통해 $dw^{[l]}$ 를 구한 후 정규화 항을 추가한다.

$$
w^{[l]}:=w^{[l]}-\alpha dw^{[l]}\\=w^{[l]}-\alpha {\{(\text{from backprop})+\frac{\lambda}{m}w^{[l]}\}}
\\=w^{[l]}-\alpha {(\text{from backprop})-\alpha \frac{\lambda}{m}w^{[l]}}
$$

- 이후 가중치 업데이트를 수행하면 해당 레이어 $l$ 의 가중치 벡터 norm 이 클 수록 $w^{[l]}$ 는 더 많이 감소된다.
- 이런 방식이기 때문에 L2 정규화는 “가중치 감소” 라고도 부른다.