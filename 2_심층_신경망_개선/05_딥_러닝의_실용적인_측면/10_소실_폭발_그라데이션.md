---
title: 소실/폭발 그라데이션
---
### 기울기 폭발 (Gradient Explosion)

- 네트워크의 각 층에서 기울기 값이 지나치게 커져서 결국 모델이 불안정해지는 현상
- 가중치가 계속 곱해지면서 결과값이 매우 커지면 그 기울기도 커지게 되고 이로 인해 모델이 올바르게 학습하지 못하며 overflow 발생 가능
- 각 층의 가중치  $W_1, W_2, W_3$ 는 $1.5$와 같다고 가정하면, 출력값은  $1.5*1.5*1.5  = 3.375$ 로 커지게 되고 층이 많아질수록 이 값은 기하급수적으로 커짐

### 기울기 소실 (Gradient Vanishing)

- 기울기가 지나치게 작아져서, 신경망이 거의 학습하지 못하는 현상
- 기울기가 너무 작아지면, 역전파 과정에서 가중치가 거의 업데이트되지 않아 학습이 멈추게 됨.

### 올바른 가중치 초기화

- 기울기 폭발과 소실을 방지하기 위해 적절한 방법으로 $W$를 초기화 해야 함