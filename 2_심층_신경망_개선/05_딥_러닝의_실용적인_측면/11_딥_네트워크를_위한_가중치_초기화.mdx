---
title: 딥 네트워크를 위한 가중치 초기화
---
![](/assets/ac2bb5ab-730c-4330-beb3-4be8facaa086.png)

- 위 네트워크에서 가중치 $w_1, w_2, w_3, w_4$ 를 적절히 초기화 해보자
- $z=w_1x_1+w_2x_2+w_3x_3+w_4x_4$ 이므로, 기울기가 폭발하는 것을 막기 위해 $w_i$ 가 작은 값을 가지길 원한다.

```python
n = 4
w = np.random.randn(n)
```

- 기존 가중치 초기화 방법
- 평균이 0이고 정규분포가 1인 가중치를 생성한다.

```python
n = 4
w = np.random.randn(n) * np.sqrt(1 / n)
```

- 새로운 가중치 초기화 방법
- 평균이 0이고 정규분포가 $\sqrt{\frac{1}{n}}$ 인 가중치를 생성한다.
- 활성 함수가 $ReLU$ 일 경우 $\sqrt{\frac{2}{n}}$ 가 더 잘 작동한다.
- 이것을 L-layer 로 확장 했을 때,  $\sqrt{\frac{2}{n^{[l-1]}}}$ 과 같이 이전 레이어의 뉴런 수를 분모로 사용한다.
- 활성 함수가 $tanh$ 일 경우 $\sqrt{\frac{1}{n^{[l-1]}}}$ 를 사용하며 이를 Xavier 초기화 라고 한다

![](/assets/9ebcc58d-1f95-425a-8889-1690ab41cfa8.png)

- 더 좁은 범위에서 $w$를 초기화 하므로써 기울기 소실과 폭발을 막는다.