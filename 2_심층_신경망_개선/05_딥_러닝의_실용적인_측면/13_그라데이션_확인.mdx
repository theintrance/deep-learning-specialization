---
title: 그라데이션 확인
---
- 그라데이션 확인을 수행하기 위해 모든 파라미터의 거대 집합을 $\theta$ 라고 한다. 즉 비용함수 $J$ 는 이렇게 표현 가능하다

$$
J(W^{[1]}, b^{[1]},W^{[2]}, b^{[2]},\dots,W^{[l]}, b^{[l]})\\=J(\theta)
$$

- 곧 가중치의 바이어스에 대한 기울기도 $d\theta$ 로 표현한다.

$$
\begin{align*}
\text{for each } i: \\
&\quad d\theta_{approx}[i] = \frac{J(\theta_1, \theta_2, \theta_3, \dots, \theta_i + \epsilon, \dots) - J(\theta_1, \theta_2, \theta_3, \dots, \theta_i - \epsilon, \dots)}{2\epsilon}
\end{align*}
$$

- $\theta$ 에 대해 미분 근사치를 얻었으니 이제 실제 $d\theta$ 와 얼마나 차이가 나는지 확인한다.

$$
\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2}
$$

- 벡터 간 거리공식을 이용해 추론 값과 실제 값이 얼마나 다른지 확인한다.
- $\epsilon$ = $10^{-7}$ 일때 거리 공식의 값이 $10^{-7}$ 정도로 나오면 잘 작동하고 있는 것으로 생각한다.
- 거리 공식의 값이 $10^{-3}$ 정도라면 버그를 의심한다.

### 예제 코드

```python
import numpy as np

def J(theta):
    # 예시 비용 함수: 단순한 2차 함수
    return np.sum(theta ** 2)

def compute_gradient(theta):
    # 실제 기울기 계산 (예시: J(theta) = sum(theta^2)의 기울기)
    return 2 * theta

def gradient_check(theta, epsilon=1e-7):
    theta = theta.flatten()
    grad_approx = np.zeros_like(theta)
    for i in range(len(theta)):
        theta_plus = np.copy(theta)
        theta_minus = np.copy(theta)
        theta_plus[i] += epsilon
        theta_minus[i] -= epsilon
        grad_approx[i] = (J(theta_plus) - J(theta_minus)) / (2 * epsilon)
    
    grad_actual = compute_gradient(theta)
    numerator = np.linalg.norm(grad_actual - grad_approx)
    denominator = np.linalg.norm(grad_actual) + np.linalg.norm(grad_approx)
    difference = numerator / denominator

    return difference

# 예시 파라미터 벡터
theta = np.random.randn(4)

# 그라데이션 확인
difference = gradient_check(theta)
print("Gradient difference:", difference)
```