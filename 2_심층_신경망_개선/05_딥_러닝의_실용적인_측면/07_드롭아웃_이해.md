---
title: 드롭아웃 이해
---
### 드롭아웃은 왜 효과가 있을까

- 드롭아웃은 레이어의 노드가 특정 확률로 탈락하는 정규화 기법이다.
- 노드 몇 개를 랜덤하게 탈락시켜서 해당 특성에 과도한 비중이 가는 것을 방지하게 된다.
- 이는 가중치를 분산시키고 결국 과적합(Overfitting)을 해소한다.

### 비용 함수와 드롭아웃

- 드롭아웃을 적용할 때, 비용 함수 $J$는 드롭아웃 없이 계산된 평균값에 기반한다. 따라서 드롭아웃 적용 후에도 비용 함수의 형태는 변하지 않는다.
- 드롭아웃이 적용된 모델의 비용 함수 $J$는 다음과 같이 정의된다:

$$

J^{\text{dropout}}(W, b) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \text{Regularization\_Term}

$$

- 드롭아웃은 학습 중에만 적용되며, 테스트 단계에서는 모든 뉴런이 활성화된다.

- **노이즈에 대한 강건성 향상**:
    - 드롭아웃은 학습 중에 랜덤 노이즈를 추가하는 것과 유사하여 모델이 노이즈에 대해 더 강건하게 만든다.
    - 이는 모델이 훈련 데이터의 특정 패턴에 과도하게 의존하지 않도록 돕는다.
- **학습 시간 동안의 랜덤성**:
    - 학습 동안 무작위로 뉴런을 드롭함으로써 모델이 매번 다른 뉴런 조합을 학습하게 된다. 이는 모델이 보다 일반화된 특징을 학습하도록 도와준다.