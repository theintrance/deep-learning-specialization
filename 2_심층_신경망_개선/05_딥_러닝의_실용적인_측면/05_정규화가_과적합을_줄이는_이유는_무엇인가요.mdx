---
title: 정규화가 과적합을 줄이는 이유는 무엇인가요?
---


$$
J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}, \dots,w^{[l]}, b^{[l]}) = \frac{1}{m}\sum_{i=1}^{m}L(\hat y^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2_F
$$



- 비용 함수에 정규화 항을 추가하므로써 가중치는 감소하는 방향으로 경사 하강이 진행됨
- 그런데 왜 이게 overfitting 을 방지할까?

1. **가중치 감소와 모델 단순화**

![](/assets/88c6010f-bc5d-4b40-adf5-09abc2ada0ff.png)

- 정규화 항의 $\lambda$ 가 클 수록 가중치는 작아지기 때문에 어떤 레이어의 특정 노드의 가중치는 0에 수렴할 수 있다.
- 가중치가 0에 수렴한다는 것은 레이어의 몇몇 노드들은 사용되지 않는 것과 동일한 효과를 준다.
- 이는 복잡한 NN 을 단순하게 만들고 훈련 세트에 대해 지나치게 편향되는 가중치를 가지게 되는 것을 방지한다.

1. **활성 함수와 선형화 효과**

![](/assets/416c20d6-1092-482c-acd1-af19ad4e7229.png)

- 또한 레이어에서 사용하는 활성 함수가 $tanh$ 일 경우에 정규화를 도입하여 가중치를 감소시키면 $z^{[l]}$의 값도 감소하게 되고 이는 $tanh(z^{[l]})=a^{[l]}$  가 선형을 띄게 만든다.
- 이 부분 또한 NN 을 단순하게 만들어 과적합을 방지한다.