---
title: 네트워크에서 활성화 정규화
---
- 입력 데이터를 $\frac{X}{\sigma}$ 로 정규화 하면  경사하강에 유리하다는 것을 알고있음
- 입력 데이터 $X$ 뿐만 아니라 같은 원리를 각 레이어의 활성화 값에 적용할 수 있다.
- $a^{[l]}$ 를 정규화 할 수는 있으나 $z^{[l]}$ 를 정규화 하는것이 더 좋다.
(어디에 정규화를 적용할지는 논쟁의 여지가 있다.)

### 배치 노름

$$
\mu=\frac{1}{m}\sum_iz^{(i)}\\
\sigma^2=\frac{1}{m}\sum_i(z^{(i)-\mu})^2\\
z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}
$$

<aside>
💡 ($z^{(i)}$ 는 $z^{[l](i)}$ 이지만 표기를 간단하게 하기 위해 생략)

</aside>

- 이렇게 하면 $z^{(i)}_{norm}$ 은 평균이 0이고 분산이 1인 값이 된다.
- 하지만 모든 은닉 층이 이렇게 되는 것을 원치 않으므로, 아래와 같은 변수를 계산한다.
    - 모든 층의 $z$ 값이 같은 평균과 분산을 갖게 된다면 활성화 값들은 군집될 것이고 이는 모델의 학습률을 저하시킨다.

$$
\tilde{z}^{(i)}=\gamma z^{(i)}_{norm}+\beta
$$

- $\gamma$ 와 $\beta$ 는 하이퍼 파라미터가 아닌 일반 파라미터로써 학습이 가능한 수치들이다.