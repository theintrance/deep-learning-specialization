---
title: 배치 노름을 신경망에 맞추기
---
- 기존 NN 의 순방향 전파에서 선형변환은 $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ 이고 활성화 값은 $a^{(i)}=g^{[l]}(z^{[l]})$ 였다면 배치 노름을 도입했을 때는 아래와 같다.

$$
z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}\\\text{ }\\\mu=\frac{1}{m}\sum_iz^{[l](i)}\\\text{ }\\
\sigma^2=\frac{1}{m}\sum_i(z^{[l](i)-\mu})^2\\\text{ }\\
z^{[l](i)}_{norm}=\frac{z^{[l](i)}-\mu}{\sqrt{\sigma^2+\epsilon}}\\\text{ }\\
\tilde{z}^{[l]}=\gamma^{[l]} z^{[l]}_{norm}+\beta^{[l]}\\\text{ }\\
a^{[l]}=g^{[l]}(\tilde{z}^{[l]})
$$

- 즉 $\gamma^{[l]}$ 와 $\beta^{[l]}$ 는 해당 레이어에 관여하는 NN 파라미터가 된다.
- 여기서의 $\beta$ 는 모멘텀, RMSprop, Adam 의 하이퍼 파라미터와는 무관하다.
- 배치 노름에서 사용되는 $\gamma^{[l]}$ 와 $\beta^{[l]}$ 도 역전파를 통한 경사 하강을 수행하게 된다.

$$
\beta^{[l]}:=\beta^{[l]}-\alpha{}d\beta^{[l]}
$$

- 배치 노름 파라미터도 $W, b$ 와 동일하게 모멘텀이나 Adam 을 사용한 경사 하강을 할 수 있다.
- 딥 네트워크 프레임워크를 사용한다면 이런 배치 노름은 직접 도입하지 않아도 된다.
    - 프레임워크에 이미 구현되어있음
        - `e.g.) tf.nn.batch_normalization`
- 배치 노름을 도입 한 경우, $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ 의 과정에서 $z^{[l]}_{norm}$ 을 계산하며 평균을 빼므로, 상수항인 $b^{[l]}$ 을 더하는 것은 아무 의미가 없어진다. 그러므로 배치 노름을 사용할 때는 $z^{[l]}=w^{[l]}a^{[l-1]}$ 로 계산 한다.
    - 또는 항상 $b^{[l]}$ 를 0으로 설정한다.
- 배치 노름의 파라미터 $\gamma^{[l]}$ 와 $\beta^{[l]}$  의 차원은 $(n^{[l]}, 1)$ 이다.

### 배치 노름의 경사 하강

```python
for x in mini_batches:
    # forward prop
    z = np.dot(W, a_prev) + b
    mu = np.mean(z, axis=0)
    sigma2 = np.var(z, axis=0)
    z_norm = (z - mu) / np.sqrt(sigma2 + epsilon)
    tilde_z = gamma * z_norm + beta
    a = g(tilde_z)
    
    # back prop
    dz = ... 
    dW = ...
    dbeta = ...
    dgamma = ...
    
    # parameter update
    W -= learning_rate * dW
    beta -= learning_rate * dbeta
    gamma -= learning_rate * dgamma
```