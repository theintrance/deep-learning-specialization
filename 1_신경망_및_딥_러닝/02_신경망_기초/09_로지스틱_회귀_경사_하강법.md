---
title: 로지스틱 회귀 경사 하강법
---

- Coursera 요약
    - Recap of logistic regression setup: Y_hat = sigma(W1X1 + W2X2 + B) and Loss = AY
    - Computation graph for logistic regression with two features, X1 and X2
    - Derivative of loss with respect to A is -Y/A + 1 - Y/1 - А
    - Derivative of Z with respect to A is A(1 - A)
    - Derivative of loss with respect to Zis A - Y
    - Derivative of W1 is X1 * DZ
    - Derivative of W2 is X2 * DZ
    - Derivative of B is DZ
    - Gradient descent for single example: compute DZ, DW1, DW2, and DB, then update W1, W2, and B with learning rate alpha

### Logistic regression recap

$$
\begin{align*}
z &= w^Tx + b\\
\hat{y}&=a=\sigma(z)\\
L(a,y)&=-(ylog(a)+(1-y)log(1-a))
\end{align*}
$$

- 학습 목표: 
단일 훈련 예제에 대해서 손실을 계산하고, 
미분하기 위해 역방향으로 이동하는 방법을 알아보자

### Logistic regression derivatives

- 두 개의 특성을 가진 로지스틱 회귀의 계산 그래프를 그리고 미분해보자.

![](/assets/53bdc320-1baa-4d08-8b90-256c672f15e3.png)

- L → a
    - da를 구해야 함
    
    $$
    da=\frac{dL(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a}
    $$
    
- a → z
    - dz를 구하기 전 먼저 da/dz를 구해야 함
    
    $$
    \frac{da}{dz}=a(1-a)
    $$
    
    - dz는 체인룰을 이용해서 구함
    
    $$
    dz = \frac{dL}{dz} = \frac{dL}{da}\frac{da}{dz} = a - y
    $$
    
- z → w1, w2
    - dw1 을 구해야 함
    
    $$
    dw_1=x_1 dz\\
    dw_2=x_2dz\\
    $$
    
- z → b
    - db를 구해야 함
    
    $$
    db = dz
    $$
    
- 마지막으로 반복문에서 다음을 수행함

$$
\begin{align*}
w_1&:=w_1-\alpha dw_1\\
w_2&:=w_2-\alpha dw_2\\
b&:=b-\alpha db
\end{align*}
$$

🚨 참고!

위에 미분 과정에서 왜 이렇게 미분되었는지는 크게 중요하지 않음

미분했을 때 저런 도함수가 나온다 정도로만 알고 있어도 충분함

위 미분 과정에 대한 자세한 내용은 아래 참조

[Derivation of DL/dz](https://community.deeplearning.ai/t/derivation-of-dl-dz/165)