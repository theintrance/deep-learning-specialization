---
title: 로지스틱 회귀 벡터화
---

학습 예제 데이터가 총 3개일때, 활성값 $a$ 는 다음 방법으로 계산함

$$
z^{(1)} = w^Tx^{(1)}+b\\
a^{(1)}=\sigma(z^{(1)})\\

z^{(2)} = w^Tx^{(2)}+b\\
a^{(2)}=\sigma(z^{(2)})\\
z^{(3)} = w^Tx^{(3)}+b\\
a^{(3)}=\sigma(z^{(3)})\\

$$

모든 학습 예제를 한번에 벡터화 해서 연산을 해야 빠름

벡터화를 하기 위해 학습 예제들의 열 벡터를 아래와 같이 대문자 X 로 정의함

$$
X = \begin{bmatrix}
x^{(1)}&x^{(2)}&x^{(3)}&...&x^{(m)}
\end{bmatrix}
$$

여기서 $X$ 의 크기는 $(n,m)$. $n$ 은 특성(feature)의 수이고, $m$ 은 학습 예제의 수 

$w$ 는 $(n,1)$ 크기, 따라서 $w^T$ 는 $(1,n)$ 벡터가 되어 $X$ 와 곱할 수 있음. 이렇게 해서 $Z$ 는 $(1,m)$ 벡터가 됨.

활성화 출력 $A$까지 이 개념을 확장하면 아래와 같이 표현할 수 있음 

$$
Z=\begin{bmatrix}
z^{(1)}&z^{(2)}&...&z^{(m)}
\end{bmatrix}=w^TX+ \begin{bmatrix}
b&b&...&b
\end{bmatrix}\\
A=\sigma(Z)
$$

여기에서 $w^TX$ 에 bias 를 더하기 위해 자연스레 상수 $b$ 가 (1 x $m$) 크기의 행렬로 확장되었는데, 이를 브로드캐스팅이라고 함

파이썬에서 이를 구현하면 아래와 같음

```python
Z = np.dot(w.T, X) + b
A = 1 / (1 + np.exp(-Z))
```