---
title: m개 예제의 경사 하강법
---

- Coursera 요약
    - Definition of cost function J: average of 1/m sum from i=1 to m of the loss when algorithm output a_i on exampley
    - Derivative of overall cost function is average of derivatives of individual loss terms
    - Initialize j=0, dw_1=0, dw_2=0, db=0
    - Use for loop over training set to compute derivative with respect to each training example and add them up
    - Compute z_i=w transpose x_i + b, prediction a_i-sigma of z_i
    - Add up J, J+=y_i log a_i + 1-y_i log 1-a_i, negative sign in front of whole thing
    - Compute dz_iFa_i-y_i, dw_I+=x_lidz_i, dw_2+=x_2idz_i, db+=dz_i
    - Divide by m to compute averages
    - Implement one step of gradient descent: w_1-=learning rate*dw_1, w_2-=learning rate*dw_2, b-=learning rate*db
    - Weaknesses: need to write two for loops
    - Vectorization techniques allow to get rid of explicit for loops

### Logistic regression on *m* examples

- 개념
    - 비용 함수 recap
        
        $$
        J(w, b)=\frac{1}{m}\sum_{i=0}^{m}{L(a^{(i)},y^{(i)})}\\
        a^{(i)}=\hat{y}^{(i)}=\sigma(z^{(i)})=\sigma(w^Tx^{(i)}+b)
        $$
        
    - 그럼 w1에 대한 도함수는?
        
        $$
        \begin{align*}
        \frac{\partial}{\partial w_1} J(w, b)=\frac{1}{m}\sum_{i=0}^{m}&\frac{\partial}{\partial w_1} {L(a^{(i)},y^{(i)})}\\
        &-------
        \end{align*}
        $$
        
    - 밑줄 친 부분은 이전 시간에 배운 dw1 (단일 훈련 예제)
    
    $$
    dw_1^{(i)} = \frac{\partial}{\partial w_1} {L(a^{(i)},y^{(i)})}
    $$
    
- 실제 구현 (summation을 for문 코드로 나타내기)
    - 초기화
        
        $$
        J=0,\ dw_1=0,\ dw_2=0,\ db=0
        $$
        
    - for 문
        
        $$
        \begin{align}
        for\ i =&0\ to\ m:\\
        z^{(i)} &= w^Tx^{(i)}+b\\
        a^{(i)} &= \sigma(z^{(i)})\\
        J &\mathrel{+}= -[y^{(i)}log(a^{(i)})+(1-y^{(i)})log(1-a^{(i)})]\\
        dz^{(i)} &= a^{(i)}-y^{(i)}\\
        dw_1 &\mathrel{+}= x_1^{(i)}dz^{(i)}\\
        dw_2 &\mathrel{+}= x_2^{(i)}dz^{(i)}\\
        db &\mathrel{+}= dz^{(i)}
        \end{align}
        $$
        
        - 참고로 6,7번 라인은 이중 for 문임.
            - 실제로는 특성 벡터 x의 개수 n 개만큼 반복함
            - 예제로 특성 벡터가 2개라고 주어졌기 때문에 2번 함
        
        $$
        \begin{align*}
        for&\ j=0\ to\ n_x:\\
        &dw_j+=x_j^{(i)}dz^{(i)}
        \end{align*}
        $$
        
    - 총합을 m으로 나누기
    
    $$
    \begin{align*}
    J\mathrel{/}=m\\
    dw_1\mathrel{/}=m\\
    dw_2\mathrel{/}=m\\
    db\mathrel{/}=m\\
    \end{align*}
    $$
    
    - 경사 하강법
        
        $$
        \begin{align*}
        w_1&:=w_1-\alpha dw_1\\
        w_2&:=w_2-\alpha dw_2\\
        b&:=b-\alpha db\\
        \end{align*}
        $$
        
    - 경사 하강을 반복하면서 가장 낮은 점으로 내려가야 하므로 위에서 말한 과정을 계속 반복해야 함.
- 이중 for 문을 사용하기 때문에 효율성이 좋지 않음
    - 큰 데이터 셋을 처리하는데 있어서 불리함
    - for 문을 사용하지 않고 처리하는 방법이 있어야 함
    - 이를 위한 벡터화에 대해 배울 예정 → 다음 챕터