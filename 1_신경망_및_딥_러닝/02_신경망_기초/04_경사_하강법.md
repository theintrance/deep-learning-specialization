---
title: 경사 하강법
---

# 그라데이션 하강

$$
J(w, b) = \frac{1}{m}\sum_{i=1}^m L(\hat y^{(i)}, y^{(i)})
$$

가설 w, b를 평가하는 비용 함수가 최소값이 되는 w, b를 찾는게 로지스틱 회귀의 목적임

그라데이션 하강은 세운 가설 w, b를 비용 함수가 작아지는 쪽으로 업데이트 하며 최소값을 찾아나가는 알고리즘

![](/assets/6c0b4bfa-682e-436d-97e5-bd156a028633.png)

편의를 위해 비용 함수를 $J(w)$ 로 가정 했을 때 $w$ 를 다음과 같이 업데이트 함

$$
w:= w-\alpha\frac{d}{dw}J(w)
$$

$(w, J(w))$ 의 포인트에서 위를 수행하면 $J$ 가 작아지는 쪽으로 포인트가 이동하게 됨

얼마나 많이 이동하는지는 $\alpha$ 값으로 조정하게 되고 이를 Learning Rate 라고 부름

해당 강의와 python 코드에서, 도함수들을 아래와 같이 표기함

$$
dw=\frac{d}{dw}J(w, b)

$$

$$
db=\frac{d}{db}J(w,b)
$$