---
title: 비선형 활성화 함수가 필요한 이유
---

### 비 선형 활성화 함수는 왜 필요할까?

$A=g(z)$ 에서 $g(x)=x$ 라면 어떨까 (이를 선형 활성화 함수 또는 항등 함수라고 함)

이를 NN 에 적용하고, 출력 레이어에서만 sigmoid 를 적용 했을 때 단일 레이어 단일 노드 로지스틱 회귀보다 표현력이 떨어진다는 것이 증명됨

$$
a^{[2]}=w^{[2]}(w^{[1]}x+b^{[1]})+b^{[2]}\\
(w^{[2]}w^{[1]})x+(w^{[2]}b^{[1]}+b^{[1]})
$$

각 레이어의 출력이 선형 함수에 대한 선형 함수가 되어버려서 히든 레이어가 쓸모 없어짐

### 그럼 선형 함수는 언제 사용?

만약 예측하고자 하는 값이 주택 가격 예측 처럼 실수 집합 안에 있을 때, 출력 레이어에 한해서 선형 함수를 쓸 수 있음. 이런 경우에도 히든 레이어의 활성 함수들은 ReLU, Leaky ReLU, tanh 처럼 비 선형 함수여야 함