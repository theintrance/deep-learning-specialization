---
title: 신경망을 위한 그라데이션 하강
---

히든 레이어 한층, 출력 레이어 한층이 있는 NN 의 경우 비용 함수는

$$
 J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})= \frac{1}{m}\sum_{i=1}^mL(a^{[2]}, y)
$$

히든 레이어의 가중치 W, 바이어스 b 와 출력 레이어 W, b가 비용 함수에 사용이 되고, 경사 하강 시 이 4가지의 가설 변수들을 업데이트 해주어야 함

$$
dZ^{[2]} = A^{[2]} - Y
$$

$$
dW^{[2]} = \frac{1}{m} dZ^{[2]} A^{[1]T}
$$

$$
db^{[2]} = \frac{1}{m} \text{np.sum}(dZ^{[2]}, \text{axis}=1, \text{keepdims}=\text{True}) 
$$

$$
dZ^{[1]} = W^{[2]T} dZ^{[2]} * g^{[1]'}(Z^{[1]})
$$

- * 은 원소별 곱

$$
dW^{[1]} = \frac{1}{m} dZ^{[1]} A^{[0]T}\\
A^{[0]}=X
$$

$$
db^{[1]} = \frac{1}{m} \text{np.sum}(dZ^{[1]}, \text{axis}=1, \text{keepdims}=\text{True})
$$