---
title: 활성화 기능
---

### 활성화 함수

지금까지 NN 을 구성할 때 sigmoid 함수만 사용 해왔는데, 말고도 여러가지 활성 함수가 있음

1. $tanh$

$$
tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
$$

![](/assets/06cd64e3-6d9a-42b3-92f2-7cd6a58ad661.png)

- sigmoid 함수의 평행이동, 스케일링 버전
- sigmoid 보다 대부분의 경우에서 더 잘 작동함
- 데이터의 평균이 0에 가까워지기 때문에 데이터를 중심에 위치시키기 유리함

1. $ReLU$

$$
ReLU(z) = max(0, z)
$$

![](/assets/a1bcec6c-21ae-47fa-b43d-f64792ba0f1b.png)

1. $Leaky\ ReLU$

$$
\alpha=0.01\\
Leaky\ ReLU(z)=max(\alpha z, z)
$$

![](/assets/30fad627-a726-4c53-84de-2cad83939da1.png)

sigmoid 함수는 대부분 출력 레이어의 활성 함수로만 사용됨

어떤 활성 함수를 사용해야할지 모를때는, 여러 활성 함수를 사용해보고 평가해 볼 것