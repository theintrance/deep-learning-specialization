---
title: 신경망 개요
---

### 신경망

![](/assets/3f70704d-6bdc-4b0a-9533-b4ae7c278b3c.png)

지금까지 배운 sigmoid 유닛 여러개를 여러 겹으로 쌓아 신경망을 구성할 수 있음

위 그림같은 경우 입력 레이어, 히든 레이어, 출력 레이어가 각각 하나씩 존재하고 각 레이어의 노드 갯수는 3, 3, 1 개임

n번째 레이어의 변수는 앞으로 아래와 같이 표기함

$$
\text{val}^{[n]}
$$

위 NN 에서 $\hat y$ 를 계산하는 방법은 아래와 같음

$$
z^{[1]}=W^{[1]}x+b^{[1]}
$$

$$
a^{[1]}=\sigma({z^{[1]}})
$$

$$
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}
$$

$$
a^{[2]}=\sigma({z^{[2]}})
$$

$$
\hat y=a^{[2]}
$$

즉 손실함수 → $L(a^{[2]}, y)$

또한 역방향 전파에서 각 레이어 변수에 대한 도함수를 구할때도 같은 표기법을 사용함

$$
dw^{[n]}, db^{[n]}, da^{[n]}, dz^{[n]}

$$